{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING MACHINE LEARNING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, we will generate a machine learning model decision tree. Some  data preparation tasks need to be performed first as follows:\n",
    "1. Replace null values with median of that column.\n",
    "2. Convert categorical variables to numerical variables.\n",
    "3. Prepare training dan test data.\n",
    "\n",
    "The kernel is based on Kaggle Tutorial: EDA & Machine Learning and Kaggle Tutorial: Your First Machine Learning Model by Hugo-Brown Anderson.\n",
    "\n",
    "In this tutorial, we will generate a decision tree model of Titanic dataset to predict the survival of its passengers.\n",
    "\n",
    "We will use the decision tree implementation from a machine learning library, Scikit Learn.\n",
    "\n",
    "You may need to download and install some libraries e.g. sklearn, seaborn etc. You can use the pip instruction to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_train = pd.read_csv('data/train-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what we have in the dataset. We will print the first 3 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First, we need to do some preprocessing on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some preprocessing to the dataset. We need to temporarily remove the target class, `Survived`, from the dataset.\n",
    "\n",
    "But first, you'll store the target variable of the training data for safe keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store target variable of training data in a safe place\n",
    "survived_train = df_train.Survived\n",
    "\n",
    "# Drop the Survived column\n",
    "data = df_train.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your new DataFrame data using the `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical variables have missing values, `Age` and `Fare`. Some categorical variables are also have missing values; `Cabin` and `Embarked`.\n",
    "\n",
    "Now we will focus on fixing the numerical variables: you will impute or fill in the missing values for those columns, using the median of these variables where you know them. Impute is a process to remove null values in your dataset by replacing them with other values.\n",
    "\n",
    "Note that in this case, you use the median because it's perfect for dealing with outliers. In other words, the median is useful to use when the distribution of data is skewed. Other ways to impute the missing values would be to use the mean, which you can find by adding all data points and dividing by the number of data points, or mode, which is the number that occurs the highest number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with median column values\n",
    "data.fillna(data.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check out the modified data. You can see all the numerical variables are now consists of non-null numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models work input features that are numerical. Hence you want to encode your data with numbers, so you'll want to change 'object' dtypes variables to numbers. You can use the pandas function `.get_dummies()` to do so. The `.get_dummies()` allows you to create a new column for each of the options in categorical variables. \n",
    "\n",
    "In this example, we would like to convert the `Sex` attribute to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_dummies()` allows you to create a new column for each of the options in 'Sex'. So it creates a new column for `female`, called `'Sex_female'`, and then a new column for `'Sex_male'`, which encodes whether that row was male or female.\n",
    "\n",
    "Now, because you added the `drop_first` argument in the line of code above, you dropped `'Sex_female'` because, essentially, these new columns, `'Sex_female'` and `'Sex_male'`, encode the same information.\n",
    "\n",
    "So all you have done is create a new column `'Sex_male'`, which has a 1 if that row is a male - and a 0 if that row is female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll select the columns `['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']` from your DataFrame to build your first machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns and view head\n",
    "data = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the entries are non-null now!\n",
    "\n",
    "Now, you have got your data in a form to build first machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a decision tree classifier? It is a tree that allows you to classify data points, which are also known as target variables, based on feature variables.\n",
    "\n",
    "In this lab, we are using the sklearn tree library to develop our decision tree classifier. Documentation of this library can be found at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting a model to your data, we split it into training and test sets.\n",
    "\n",
    "The training set will be used for the decision tree to learn the dataset. It will output a model or classifier.\n",
    "\n",
    "The test data will be used to test how good the genefrated classifier is.\n",
    "\n",
    "Split the dataset into train and test data. Lets set the test size to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "train_label = survived_train[msk]\n",
    "test = data[~msk]\n",
    "test_label = survived_train[~msk]\n",
    "\n",
    "print('Number of records in dataset: ', len(data))\n",
    "print('Number of records in training set: ', len(train))\n",
    "print('Number of frecords in test set: ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use scikit-learn, which requires your data as arrays, not DataFrames so transform them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.values\n",
    "y = train_label.values\n",
    "\n",
    "X_test = test.values\n",
    "y_test = test_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data. Note that you name your model clf, which is short for \"Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatntiate model and fit to data\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature variables X is the first argument that you pass to the `.fit()` method, while the target variable, y, is the second argument.\n",
    "\n",
    "The output tells you all that you need to know about your Decision Tree Classifier that you just built: as such, you see, for example, that the max depth is set at 3.\n",
    "\n",
    "Now, you'll make predictions on your training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute accuracy on the training set\n",
    "train_accuracy = clf.score(X, y)\n",
    "\n",
    "#Compute accuracy on the testing set\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print('Training accuracy: ', train_accuracy)\n",
    "print('Test accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the test_accuracy stores the accuracy of the generated classifier. named clf, in predicting the survival of the passengers listed in the test set.\n",
    "\n",
    "From the generated result, the training set produced an accuracy at 83.4% while test accuracy is at 80.9%. \n",
    "\n",
    "You may improve the accuracy by choosing the suitable `max_depth`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `max_depth`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth of the tree is known as a hyperparameter, which means a parameter you need to decide before you fit the model to the data. If you choose a larger max_depth, you'll get a more complex decision boundary.\n",
    "\n",
    "    If your decision boundary is too complex, you can overfit to the data, which means that your model will be describing noise as well as signal.\n",
    "\n",
    "    If your max_depth is too small, you might be underfitting the data, meaning that your model doesn't contain enough of the signal.\n",
    "\n",
    "So how  do you identify the best `max_depth`?\n",
    "\n",
    "One way is to hold out a test set from your training data. You can then fit the model to your training data, make predictions on your test set and see how well your prediction does on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the generated tree.\n",
    "\n",
    "We can plot the tree with the `plot_tree` function.\n",
    "\n",
    "You may need to install a graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot_data = StringIO('tree.dot')\n",
    "from sklearn.tree import export_graphviz\n",
    "import io\n",
    "from graphviz import Source\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG\n",
    "from IPython.display import display\n",
    "\n",
    "export_graphviz(clf, out_file='tree.dot', filled=True, \n",
    "                rounded=True, special_characters=True, class_names=['0','1'])\n",
    "\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you have:\n",
    "1. Prepare the data in a form to build a machine learning model.\n",
    "2. Built a machine learning model using a decision tree classifier.\n",
    "3. Makes prediction based on the generated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded assessment\n",
    "\n",
    "## Predict if the unknown passengers are survived/ not survived.\n",
    "\n",
    "These are ungraded questions, but your submission will be used as a proof of attendance. Different from the previous lab, submission that produced the best accuracy of prediction will be rewarded. You may form a group of two. I will select three winners, which will be determined based on the accuracy of the prediction, correctness of the code and the time that you need to submit your answer. All the submissions must be accompanied by the runnable code that produced the prediction.\n",
    "\n",
    "Please use the test data provided, 'test-no-label.csv'. Your group are required to predict the 'Survived' label of each of the passenger listed in the test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
